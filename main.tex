\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{url}
\usepackage{multicol}
\usepackage[round]{natbib}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}

\title{Bayesian Meta Analysis with Multiple Data Sources}
\author{}
\date{}

\begin{document}

\maketitle


\paragraph{Introduction} In many areas of health science research, it is common that multiple studies are conducted on multiple observational data sources to investigate similar research questions, 
and it is also of interest to combine and summarize estimates of a similar quantity obtained from multiple studies. 
However, there are concerns about biases due to systematic errors in observational data, and previous works have introduced the use 
of negative controls to detect such biases and adjust the estimates accordingly \citep{lipsitch2010negative,arnold2016brief,schuemie2018empirical}. 
Our goal is to combine available estimates from multiple studies while adjusting for biases based on analyses of negative controls. 
We accomplish this through a Bayesian hierarchical modeling framework. 


\paragraph{Related work} We build on recent work of combining estimates obtained from multiple sources \citep{yao2021bivariate} 
and extend existing work that performs calibration on estimates obtained from a single data source \citep{mulgrave2020bayesian}.

\paragraph{The model}
We develop a Bayesian hierarchical model to obtain combined estimates from multiple sources 
while addressing the systematic errors that induce biases at different sources using negative control outcomes.  
We assume that the estimated quantities of interest from different sources are the same or equivalent, even though the study designs adopted at those sources can be different. 
For example, researchers may choose to adopt the case-control design or self-control design depending on the characteristics of different data sources, 
but the estimated quantity should all be the (log) odds ratio or (log) relative risk of a certain outcome between two exposures.

Let $\theta \in \mathbb{R}$ denote the common quantity of interest - for simplicity, we will refer to this quantity as the ``effect'' to estimate. 
Suppose there are $N$ data sources (indexed by $i$), and for source $i$, let the true effect be $\theta_i$. 
Due to the systematic errors, the obtained estimate $\tilde{\theta}_i$ on source $i$ can be biased, with the true bias being an unknown quantity $\beta_i$. 
Our goal is to acquire a summary estimate $\theta$ 
while adjusting for bias $\beta_i$ at source $i$ (and thus effectively also de-biasing the estimate $\tilde\theta_i$).

Assume the following hierarchical normal model for $\theta_i$ and $\tilde\theta_i$:
\begin{align}
    \theta_i &\sim N(\theta, \gamma^2),\\
    \tilde\theta_i &\sim N(\theta_i + \beta_i, \omega_i^2).
\end{align}
Here $\gamma^2$ characterizes the between-data-source variation of the (log) effect, 
and $\omega_i^2$ is the (estimated) variance of the estimate $\tilde\theta_i$.

%Let $\theta$ be the parameter of interest (e.g., the log odds ratio, log relative risk, etc.). Suppose we have $N$ different data sources (indexed by $i$), and have chosen  Analysis on data source $i$ yields an effect estimate $\tilde{\theta}_i$ which could be biased (note that we don't necessarily run the same analysis on all data sources), while we denote the \emph{true} effect on data source $i$ by $\theta_i$ and the true bias by $\beta_i$.
To adjust for the biases, we select $M$ negative controls (indexed by $j$) used in analyses across the data sources.
Let $y_{ij}$ denote the estimated effect for the $j$th negative control on the $i$th data source, 
and let $\tau_{ij}$ be the estimated standard error for this estimate. 
Denote the true bias for the estimate $y_{ij}$ by $b_{ij}$ (i.e., if there is no bias, $b_{ij} = 0$). 
Again, assume a hierarchical normal model for the negative control estimates:
\begin{align}
    b_{ij} &\sim N(\mu_i, \sigma_i^2),\\
    y_{ij} &\sim N(b_{ij}, \tau_{ij}^2).
\end{align}
Here $\mu_i$ represents the average systematic bias for the $i$th data source. 
Moreover, assume that the bias of the data-specific estimate also follows the bias distribution
\begin{equation}
    \beta_i \sim N(\mu_i, \sigma_i^2).
\end{equation}

\paragraph{Inference} 
The key parameters of interest include $\theta$ (the summary quantity) and also the $\theta_i$'s (the source-specific true effect). 
We use a Markov chain Monte Carlo inference scheme, more specifically a Gibbs sampler, to conduct Bayesian inference to estimate these parameters as follows.

Assume that the variance terms $\omega_i^2$ and $\tau_{ij}^2$ are known (or reliably estimated). 
Then the unknown quantities of the model include $\{\theta, \gamma^2, \theta_i, \beta_i, \mu_i, \sigma_i^2, b_{ij}\}$. 
We can adopt the following semi-conjugate priors:
\begin{itemize}
    \item $\theta \sim N(\theta_0, \epsilon_{\theta}^2)$ and $\mu_i \sim N(\mu_0, \epsilon_{\mu}^2)$;
    \item $\gamma^2 \sim \text{inv-Gamma}(u_0/2, u_0\gamma^2_0/2)$, and $\sigma_i^2 \sim \text{inv-Gamma}(\nu_0/2, \nu_0\sigma^2_0/2)$.
\end{itemize}

With the above semi-conjugate priors, a Gibbs sampler can be easily implemented to approximate the posterior distributions of all unknown quantities. The following steps shall be run iteratively:
\begin{enumerate}
    \item update $b_{ij}$ conditioned on $\sigma_i^2$ and $\mu_i$: $b_{ij} \sim N\left(\frac{\sigma_i^2y_{ij}+\tau_{ij}^2\mu_i}{\sigma_i^2+\tau_{ij}^2}, \frac{\sigma_i^2\tau_{ij}^2}{\sigma_i^2+\tau_{ij}^2}\right)$;
    \item update $\sigma_i^2$ conditioned on $b_{ij}$ and $\mu_i$: $\sigma_i^2 \sim \text{inv-Gamma}\left(\frac{\nu_0+M+1}{2}, \frac{\nu_0\sigma_0^2 + \sum_{j=1}^M(b_{ij}-\mu_i)^2+(\beta_i-\mu_i)^2}{2}\right)$;
    \item update $\mu_i$ conditioned on $b_{ij}, \sigma_i^2, \theta_i$: $\mu_i \sim N\left(\frac{\epsilon_{\mu}^2(\sum_{j=1}^M b_{ij}+\beta_i)+\sigma_i^2\mu_0}{(M+1)\epsilon_{\mu}^2 + \sigma_i^2},\frac{\sigma_0^2\epsilon_{\mu}^2}{(M+1)\epsilon_{\mu}^2 + \sigma_i^2}\right)$;
    \item update $\beta_i$ conditioned on $\mu_i, \theta_i, \sigma_i^2$: $\beta_i \sim N\left(\frac{\sigma_i^2(\tilde\theta_i - \theta_i)+\omega_i^2\mu_i}{\sigma_i^2+\omega_i^2}, \frac{\sigma_i^2\omega_i^2}{\sigma_i^2+\omega_i^2}\right)$;
    \item update $\theta_i$ conditioned on $\beta_i, \theta, \gamma^2$: $\theta_i \sim N\left(\frac{\gamma^2(\tilde\theta_i-\beta_i)+\omega_i^2\theta}{\gamma^2+\omega_i^2},\frac{\gamma^2\omega_i^2}{\gamma^2+\omega_i^2}\right)$;
    \item update $\theta$ conditioned on $\theta_i$ and $\gamma^2$: $\theta \sim N\left(\frac{\epsilon_{\theta}^2 \sum_{i=1}^N \theta_i + \gamma^2 \theta_0}{N\epsilon_{\theta}^2 + \gamma^2},\frac{\gamma^2 \epsilon_{\theta}^2}{N\epsilon_{\theta}^2 + \gamma^2}\right)$;
    \item update $\gamma^2$ conditioned on $\theta$ and $\theta_i$: $\gamma^2 \sim \text{inv-Gamma}\left(\frac{u_0+N}{2}, \frac{u_0\gamma_0^2 + \sum_{i=1}^N (\theta_i - \theta)^2}{2}\right)$.
\end{enumerate}

\paragraph{Discussion} 
We have assumed normal distributions for between-source variability and the biases throughout for simplicity 
and also based on previous findings that normal approximations seem to work well in practice \citep{schuemie2018empirical,mulgrave2020bayesian}. 
However, for cases where the normality assumption doesn't hold (e.g., when the negative control estimates have multi-modal or heavy-tail distributions, or effects from different sources have large dispersion), 
non-normal models could be considered within the hierarchical model framework. 
For example, we may extend the normal models to normal mixture models or $t$ distributions. 
Further, we may consider different transformations for other types of quantities of interest - 
for example, if the parameter to estimate is a proportion, then we can adopt a logit-normal model or a Beta distribution instead of the log-normal model assumed here.

\newpage
\bibliographystyle{chicago}
\bibliography{ref}


\end{document}
