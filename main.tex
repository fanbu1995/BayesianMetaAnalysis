\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{url}
\usepackage{multicol}
\usepackage[round]{natbib}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}

\title{Bayesian Meta Analysis with Multiple Data Sources}
\author{Fan Bu, George Hripcsak, David Madigan, Aki Nishimura, Patrick Ryan, \\Martijn Schuemie, and Marc Suchard}
\date{September 2021}

\begin{document}

\maketitle


\paragraph{Introduction} In many areas of health science research, it is common that multiple studies are conducted on multiple observational data sources to investigate similar research questions,
and it is also of interest to combine and summarize results across multiple studies.
More importantly, different studies may employ different designs to estimate the effect.
In addition, there are concerns about biases due to systematic errors in observational data,
and previous works have introduced the use of negative controls to detect such biases and adjust the effect estimates accordingly \citep{lipsitch2010negative,arnold2016brief,schuemie2018empirical}.
Our goal is to combine multiple studies while adjusting for biases based on analyses of negative controls in order to produce a more robust estimate for a quantity of interest.
We accomplish this through a Bayesian hierarchical modeling framework.


\paragraph{Related work}
We build on recent work of combining estimates obtained from multiple sources \citep{yao2021bivariate}
and extend existing work that performs calibration on estimates obtained from a single data source \citep{mulgrave2020bayesian}.
We note that existing work of estimate combination and/or empirical calibration has heavily focused on adjusting available parameter point estimates,
but in this work we wish to address a more general case where we directly combine information from the likelihood functions (or proper approximation of the likelihoods) of the parameter of interest across data sources.

\paragraph{The model}
We develop a Bayesian hierarchical model to obtain combined estimates from multiple sources
while adjusting for the systematic errors that induce biases at different sources using negative control outcomes.
We assume that the effects (or parameters) of interest at different data sources are the same or equivalent, even though the study designs adopted at those sources can be different.
For example, researchers may choose the case-control design or self-control design depending on the characteristics of different data sources,
but the quantity of interest should all be the (log) odds ratio or (log) relative risk of a certain outcome between two exposures.

Let $\theta \in \mathbb{R}$ denote the common quantity of interest - for simplicity, we will refer to this quantity as the ``effect'' to estimate.
Suppose there are $N$ data sources (indexed by $i$), and for source $i$, let the true effect be $\theta_i$.
Due to systematic errors, observational data at source $i$ can only provide us with information 
regarding a ``biased'' effect $\tilde{\theta}_i = \theta_i + b_i$, where the bias related to estimation of $\theta_i$ is unknown. 
We note that we are \textbf{not} simply working with a point estimate of $\tilde{\theta}_i $ and some uncertainty quantification (e.g., an MLE estimate and a standard error estimate), 
but rather, we possess a full likelihood function $f_i(\tilde{\theta}_i)$ with respect to the data analyzed from data source $i$. 
In the case that the exact likelihood function $f_i(\cdot)$ is too complicated, we may work with a reasonable approximation. 

Our goal is to acquire a summary estimate $\theta$
while adjusting for the source-specific bias $b_i$ (and thus effectively acquiring de-biased estimates for $\theta_i$'s).
We assume the following hierarchical structure for the source-specific (true) effect $\theta_i$'s:
\begin{equation}
\label{eq: theta_i-dist}
    \theta_i \sim N(\theta, \gamma^2),
\end{equation}
which also means
\begin{equation}
    \tilde\theta_i \sim N(\theta+b_i, \gamma^2).
\end{equation}

To adjust for the biases, $M$ negative controls (indexed by $j$) are selected and used in analyses across the data sources.
Let $y_{ij}$ denote the estimated effect (i.e., estimated bias) for the $j$th negative control on the $i$th data source. Suppose that $y_{ij}$ follows a bias distribution $P_i$ with density function $p_i(\cdot)$.
We further assume that the bias $b_i$ also follows the same distribution.
Denote the parameters of this distribution by $\boldsymbol\mu_i$.
For example, if we believe that the bias distribution is normal,
then $\boldsymbol\mu_i = (\beta_i, \tau_i^2)$ where $\beta_i$ is the mean bias and $\tau_i^2$ is the variance.


\paragraph{Inference}
The key parameters of interest include $\theta$ (the summary quantity) and also the $\theta_i$'s (the source-specific true effect).
Additionally, the quantities $b_i, \gamma^2, \boldsymbol\mu$ are also unknown.
Next we describe the Bayesian inference scheme for estimating the model parameters.
Let $\pi_0$ and $\pi_i$ denote the priors we adopt for $(\theta,\gamma^2)$ and $\boldsymbol\mu_i$, respectively.
For instance, we can use the normal-inverse-Gamma prior for $\pi_0$ and do the same for $\pi_i$ if we assume a normal bias distribution. Another possible choice for $\pi_0$ is to adopt the half-Cauchy (``Horseshoe'') prior for $\gamma^2$ for better shrinkage (an implemented sampler can be seen in \cite{nishimura2019regularization}). 

We may consider a Markov chain Monte Carlo inference scheme or simply use Hamiltonian Monte Carlo. The MCMC algorithm is sketched as follows:

In each iteration, do:
\begin{enumerate}
    \item sample $b_i$ from its full conditional $\propto p_i(b_i)\text{pnorm}(\tilde\theta_i; \theta+b_i, \gamma^2)$;
    \item sample $\tilde\theta_i$ from its full conditional $\propto f_i(\tilde\theta_i)\text{pnorm}(\tilde\theta_i; \theta+b_i, \gamma^2)$;
    \item sample $\theta, \gamma^2$ for their full conditional $\propto \pi_0(\theta, \gamma^2) \prod_{i=1}^N \text{pnorm}(\tilde\theta_i-b_i; \theta, \gamma^2)$;
    \item sample $\boldsymbol\mu$ from full conditional $\propto \pi_i(\boldsymbol\mu_i) p_i(b_i; \boldsymbol\mu_i) \prod_{j=1}^M p_i(y_{ij}; \boldsymbol\mu_i)$.
\end{enumerate}
Here ``$\text{pnorm}$'' represents the normal density function.
Steps 1 and 2 of this algorithm are very similar to the Bayesian inference scheme described in \cite{schuemie2021combining}, which have already been implemented in the \texttt{R} package \texttt{EvidenceSynthesis}. If the likelihood function $f_i$ is log-concave, an adaptive rejection sampler \citep{gilks1992adaptive} may be considered. 
If a normal model for the biases is assumed (i.e., $P_i$ is normal),
then Steps 1, 3 and 4 become trivial updates of hierarchical normal models if we adopt semi-conjugate normal-inverse-Gamma priors;
in this simplified scenario, the only potential bottleneck is in Step 2, where a Metropolis-Hastings step may be necessary.

Furthermore, it is possible to directly utilize an estimated (empirical) bias distribution $P_i$ if we so wish.
In this case, we can keep density function $p_i$ fixed and only run the first three steps in the above algorithm.

We note that there is no closed-form full conditional distribution for $\theta_i$ (the unbiased effect at source $i$), and thus we do not directly sample it in the above MCMC algorithm. 
However, a posterior distribution for $\theta_i$ can be easily approximated by a set of constructed samples $\{\theta_i^{(s)}\}$ using $\theta_i^{(s)} = \tilde\theta_i^{(s)} - b_i^{(s)}$, 
where $\tilde\theta_i^{(s)}$ and $b_i^{(s)}$ are the $s$th obtained samples for $\tilde\theta_i$ and $b_i$ , respectively. 

\paragraph{Discussion}
We have assumed a normal distribution in \eqref{eq: theta_i-dist} for simplicity and
also based on previous findings that normal distributions (or approximations) seem to work well in practice \citep{schuemie2018empirical,mulgrave2020bayesian}.
However, for cases where the normality assumption doesn't hold (e.g., when effects from different sources have a larger dispersion),
non-normal models like normal mixtures or $t$ distributions could be considered.
Further, we may consider different transformations for other types of quantities of interest -
for example, if the parameter to estimate is a proportion, then we can adopt a logit-normal model or a Beta distribution instead of the log-normal model assumed here.

\newpage
\bibliographystyle{chicago}
\bibliography{ref}


\end{document}
